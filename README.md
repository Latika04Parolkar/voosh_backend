# ğŸ§  Voosh Backend

This is the backend service for the Voosh AI news assistant.  
It fetches news articles, processes them into semantic chunks, embeds them, stores them in ChromaDB, and answers user queries using Gemini.

---

## ğŸš€ Features

- âœ… Fetches articles from RSS feeds (BBC, Reuters, etc.)
- âœ… Parses and chunks content intelligently
- âœ… Embeds using Jina Embeddings via Portkey API
- âœ… Stores semantic vectors in ChromaDB (hosted)
- âœ… Gemini-powered Q&A on the news content
- âœ… Uses Redis for session-based chat history
- âœ… RESTful API with `/chat`, `/chat/history`, `/chat/reset`

---

## ğŸ“ Folder Structure

```
voosh_backend/
â”‚
â”œâ”€â”€ app.js                     # Express server entry point
â”œâ”€â”€ .env                      # Local environment variables (ignored by Git)
â”œâ”€â”€ .env.example              # Sample env file for setup
â”œâ”€â”€ routes/
â”‚   â””â”€â”€ chat.js               # API routes for chat, history, reset
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ gemini.js             # Query Gemini via Portkey
â”‚   â””â”€â”€ redis.js              # Redis client setup
â”‚
â”œâ”€â”€ embeddings/
â”‚   â””â”€â”€ jina_embedder.js      # Text embedding using Portkey & Jina
â”‚
â”œâ”€â”€ news_ingest/
â”‚   â”œâ”€â”€ fetch_articles.js     # Fetch and save RSS articles
â”‚   â”œâ”€â”€ process_articles.js   # Chunk articles into vector-ready format
â”‚   â””â”€â”€ index_chunks.js       # Add chunks to ChromaDB
â”‚
â”œâ”€â”€ vector_store/
â”‚   â””â”€â”€ chroma_client.js      # ChromaDB client and operations
â”‚
â””â”€â”€ package.json
```

---

## âš™ï¸ Environment Setup

Create a `.env` file based on the following:

```
PORT=3001
PORTKEY_API_KEY=your_portkey_api_key
CHROMA_API_KEY=your_chroma_api_key
CHROMA_TENANT=your_chroma_tenant
CHROMA_DATABASE=your_chroma_database
REDIS_URL=redis://localhost:6379
```

Create it by copying:

```
cp .env.example .env
```

---

## ğŸ“¦ Install Dependencies

```
npm install
```

---

## ğŸ§ª Running Locally

### 1. Start the Server

```
npm run dev
```

The backend will run on:  
`http://localhost:3001`

---

## ğŸ“° Article Ingestion Workflow

### Step 1: Fetch Articles

```
node news_ingest/fetch_articles.js
```

- Saves top news articles as `.json` files in `sample_articles/`

### Step 2: Chunk the Articles

```
node news_ingest/process_articles.js
```

- Outputs `processed_chunks.json` for embedding

### Step 3: Index into ChromaDB

```
node news_ingest/index_chunks.js
```

- Embeds and stores semantic chunks

---

## ğŸ§  API Endpoints

### `POST /chat`

Send a user query, get AI answer based on retrieved news chunks.

**Request:**

```json
{
  "sessionId": "abc123",
  "message": "What's the latest from BBC?"
}
```

**Response:**

```json
{
  "response": "Gemini-generated answer..."
}
```

---

### `GET /chat/history/:sessionId`

Fetch past conversation from Redis.

---

### `POST /chat/reset`

Reset session.

```json
{
  "sessionId": "abc123"
}
```
## ğŸ™ Credits

- ğŸ§© Embeddings: Jina via Portkey
- ğŸ§  LLM: Gemini via Portkey
- ğŸ§  Vector DB: ChromaDB Cloud
- âš¡ Server: Express.js
- ğŸ’¬ Cache/Chat History: Redis
